import streamlit as st
import time
import random
from crawler import fetch_list_page, extract_detail_urls, fetch_detail_page, extract_elements
from utils import results_to_csv

st.set_page_config(page_title="èª°ã§ã‚‚ä½¿ãˆã‚‹Webã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«", layout="wide")
st.title("èª°ã§ã‚‚ä½¿ãˆã‚‹Webã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")

st.markdown("### ã‚¯ãƒ­ãƒ¼ãƒ«æ¡ä»¶ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
list_url = st.text_input("ä¸€è¦§ãƒšãƒ¼ã‚¸URLï¼ˆ<<PAGE>>ã§ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŒ‡å®šï¼‰", "https://example.com/list?page=<<PAGE>>")
num_pages = st.number_input("ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹ä¸€è¦§ãƒšãƒ¼ã‚¸æ•°ï¼ˆä¾‹: 5 â†’ 5ãƒšãƒ¼ã‚¸åˆ†ã®å…¨è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ï¼‰", min_value=1, max_value=100, value=5)
detail_selector = st.text_input("è©³ç´°ãƒšãƒ¼ã‚¸URLæŠ½å‡ºç”¨CSSã‚»ãƒ¬ã‚¯ã‚¿ï¼ˆ<<NUM>>ã§å¯å¤‰æŒ‡å®šå¯ï¼‰", "div.card > a")

# <<NUM>>ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ç¯„å›²å…¥åŠ›æ¬„ã‚’è¡¨ç¤º
num_range_start, num_range_end = None, None
if "<<NUM>>" in detail_selector:
    st.markdown(":blue[â€» <<NUM>> ã‚’å«ã‚€å ´åˆã€ç¯„å›²æŒ‡å®šã§è¤‡æ•°ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã§ãã¾ã™]")
    col_num1, col_num2 = st.columns(2)
    with col_num1:
        num_range_start = st.number_input("<<NUM>> é–‹å§‹å€¤", min_value=1, max_value=100, value=2, key="num_start")
    with col_num2:
        num_range_end = st.number_input("<<NUM>> çµ‚äº†å€¤", min_value=1, max_value=100, value=5, key="num_end")

st.markdown("#### è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºã—ãŸã„è¦ç´ ãƒªã‚¹ãƒˆï¼ˆæœ€å¤§20é …ç›®ï¼‰")
element_count = st.slider("æŠ½å‡ºè¦ç´ æ•°", min_value=1, max_value=20, value=2)

selectors = []
for i in range(element_count):
    col1, col2 = st.columns([3, 7])
    with col1:
        name = st.text_input(f"è¦ç´ å{i+1}", f"è¦ç´ {i+1}", key=f"name_{i}")
    with col2:
        selector = st.text_input(f"CSSã‚»ãƒ¬ã‚¯ã‚¿{i+1}", "", key=f"selector_{i}")
    selectors.append({"name": name, "selector": selector, "type": "css"})

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã§çµæœã¨CSVã‚’ä¿æŒ
def get_session_state():
    if 'results' not in st.session_state:
        st.session_state['results'] = []
    if 'csv_bytes' not in st.session_state:
        st.session_state['csv_bytes'] = None
    if 'columns' not in st.session_state:
        st.session_state['columns'] = []
get_session_state()

submitted = st.button("ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹")

progress_text = st.empty()
progress_bar = st.progress(0)

if submitted:
    st.info("ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’é–‹å§‹ã—ã¾ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")
    results = []
    error_count = 0
    detail_urls_set = set()
    total_detail_count = 0
    try:
        for page in range(1, num_pages + 1):
            page_url = list_url.replace("<<PAGE>>", str(page))
            progress_text.markdown(f"<span style='color:blue'>ä¸€è¦§ãƒšãƒ¼ã‚¸å–å¾—ä¸­: {page_url}</span>", unsafe_allow_html=True)
            list_html = fetch_list_page(page_url)
            if not list_html:
                st.warning(f"ä¸€è¦§ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {page_url}")
                error_count += 1
                continue
            # ã‚»ãƒ¬ã‚¯ã‚¿ç”Ÿæˆ
            selectors_to_use = []
            if "<<NUM>>" in detail_selector and num_range_start is not None and num_range_end is not None:
                for n in range(int(num_range_start), int(num_range_end) + 1):
                    selectors_to_use.append(detail_selector.replace("<<NUM>>", str(n)))
            else:
                selectors_to_use = [detail_selector]
            # å„ã‚»ãƒ¬ã‚¯ã‚¿ã§è©³ç´°URLæŠ½å‡º
            detail_urls = []
            for sel in selectors_to_use:
                detail_urls += extract_detail_urls(list_html, sel, base_url=page_url.split("/list")[0])
            for idx, d_url in enumerate(detail_urls):
                if d_url in detail_urls_set:
                    continue
                detail_urls_set.add(d_url)
                progress_text.markdown(f"<span style='color:green'>è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—ä¸­: {d_url} ({len(results)+1}ä»¶ç›®)</span>", unsafe_allow_html=True)
                d_html = fetch_detail_page(d_url)
                if not d_html:
                    st.warning(f"è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {d_url}")
                    error_count += 1
                    continue
                elements = extract_elements(d_html, selectors)
                row = {"è©³ç´°ãƒšãƒ¼ã‚¸URL": d_url}
                row.update(elements)
                results.append(row)
                total_detail_count += 1
                progress_bar.progress(min(1.0, (total_detail_count / (num_pages * max(1, len(detail_urls))))))
                time.sleep(random.uniform(1, 2))  # 1ï½2ç§’ãƒ©ãƒ³ãƒ€ãƒ ã‚¹ãƒªãƒ¼ãƒ—
        st.success(f"ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†ï¼å–å¾—ä»¶æ•°: {len(results)} / ã‚¨ãƒ©ãƒ¼: {error_count}")
        if results:
            columns = ["è©³ç´°ãƒšãƒ¼ã‚¸URL"] + [s["name"] for s in selectors]
            csv_bytes = results_to_csv(results, columns)
            st.session_state['results'] = results
            st.session_state['csv_bytes'] = csv_bytes
            st.session_state['columns'] = columns
        else:
            st.warning("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# çµæœãŒã‚ã‚Œã°å¸¸ã«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
display_results = st.session_state.get('results', [])
display_csv = st.session_state.get('csv_bytes', None)
display_columns = st.session_state.get('columns', [])
if display_results and display_csv and display_columns:
    st.download_button("CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=display_csv, file_name="crawl_results.csv", mime="text/csv")
    st.dataframe(display_results)

st.markdown("""
---
#### ğŸ“ è£œè¶³ãƒ»æ‹¡å¼µãƒã‚¤ãƒ³ãƒˆ
- JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãŒå¿…è¦ãªå ´åˆã¯ã€`crawler.py`ã®`fetch_list_page`/`fetch_detail_page`ã‚’Seleniumå®Ÿè£…ã«å·®ã—æ›¿ãˆå¯èƒ½ã§ã™ã€‚
- XPathå¯¾å¿œã¯`extract_elements`é–¢æ•°ã«`lxml`ã‚„`parsel`ã‚’ä½¿ã£ã¦è¿½åŠ ã§ãã¾ã™ã€‚
- ä¾‹å¤–å‡¦ç†ã‚„ãƒªãƒˆãƒ©ã‚¤ã€User-AgentæŒ‡å®šã€ãƒ—ãƒ­ã‚­ã‚·å¯¾å¿œãªã©ã‚‚æ‹¡å¼µå¯èƒ½ã§ã™ã€‚
- ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆã¯`crawler.py`ã®`__main__`ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
""")
